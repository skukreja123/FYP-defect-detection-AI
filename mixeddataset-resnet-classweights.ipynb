{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10825816,"sourceType":"datasetVersion","datasetId":6722282}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:35.168210Z","iopub.execute_input":"2025-03-16T18:43:35.168616Z","iopub.status.idle":"2025-03-16T18:43:40.120899Z","shell.execute_reply.started":"2025-03-16T18:43:35.168575Z","shell.execute_reply":"2025-03-16T18:43:40.119917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset, Dataset, random_split\nfrom torchvision import models, transforms\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.manifold import TSNE  \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport os\nfrom torch.optim.lr_scheduler import CosineAnnealingLR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:40.121832Z","iopub.execute_input":"2025-03-16T18:43:40.122353Z","iopub.status.idle":"2025-03-16T18:43:47.028667Z","shell.execute_reply.started":"2025-03-16T18:43:40.122322Z","shell.execute_reply":"2025-03-16T18:43:47.027836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FabricDefectDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = os.listdir(root_dir)\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n        self.images = self._load_images()\n\n    def _load_images(self):\n        images = []\n        for cls in self.classes:\n            cls_dir = os.path.join(self.root_dir, cls)\n            for img_name in os.listdir(cls_dir):\n                img_path = os.path.join(cls_dir, img_name)\n                images.append((img_path, self.class_to_idx[cls]))\n        return images\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path, label = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.030510Z","iopub.execute_input":"2025-03-16T18:43:47.030955Z","iopub.status.idle":"2025-03-16T18:43:47.039772Z","shell.execute_reply.started":"2025-03-16T18:43:47.030926Z","shell.execute_reply":"2025-03-16T18:43:47.038719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.041808Z","iopub.execute_input":"2025-03-16T18:43:47.042107Z","iopub.status.idle":"2025-03-16T18:43:47.060950Z","shell.execute_reply.started":"2025-03-16T18:43:47.042081Z","shell.execute_reply":"2025-03-16T18:43:47.060039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = FabricDefectDataset(root_dir='/kaggle/input/fabric-defect-dataset/FYP/Fabric Defects Dataset/Fabric Defect Dataset', transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.061920Z","iopub.execute_input":"2025-03-16T18:43:47.062191Z","iopub.status.idle":"2025-03-16T18:43:47.082971Z","shell.execute_reply.started":"2025-03-16T18:43:47.062166Z","shell.execute_reply":"2025-03-16T18:43:47.082143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.084010Z","iopub.execute_input":"2025-03-16T18:43:47.084284Z","iopub.status.idle":"2025-03-16T18:43:47.171107Z","shell.execute_reply.started":"2025-03-16T18:43:47.084258Z","shell.execute_reply":"2025-03-16T18:43:47.170284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = np.zeros(len(dataset.classes))\nfor _, label in dataset.images:\n    class_counts[label] += 1\n\nclass_weights = 1.0 / class_counts  # Inverse of class counts\nclass_weights = class_weights / class_weights.sum()  # Normalize weights\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.171756Z","iopub.execute_input":"2025-03-16T18:43:47.172047Z","iopub.status.idle":"2025-03-16T18:43:47.455503Z","shell.execute_reply.started":"2025-03-16T18:43:47.172021Z","shell.execute_reply":"2025-03-16T18:43:47.454520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create DataLoaders for training and test sets\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Load pre-trained ResNet18 model\nmodel = models.resnet18(pretrained=True)\n\n# Fine-tuning: Unfreeze more layers\nfor param in model.parameters():\n    param.requires_grad = True  # Unfreeze all layers\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(dataset.classes))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function with class weights\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Define optimizer (using best hyperparameters)\noptimizer = optim.AdamW(model.parameters(), lr=0.0001)  # Best hyperparameters\n\n# Learning rate scheduler\nscheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=0.00001)\n\n# Training loop\nnum_epochs = 15\ntrain_losses, test_losses, train_accs, test_accs = [], [], [], []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    train_loss = running_loss / len(train_loader)\n    train_acc = 100. * correct / total\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    \n    # Evaluate on the test set\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    test_loss /= len(test_loader)\n    test_acc = 100. * correct / total\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n    \n    # Learning rate scheduler step\n    scheduler.step()\n\n# Save the trained model\ntorch.save(model.state_dict(), 'mixeddataset_resnet_classweights.pth')\nprint(\"Model saved as 'mixeddataset_resnet_classweights.pth'\")\n\n\n# Evaluate on the test set\nmodel.eval()\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\ntrue_labels, predicted_labels, predictions = [], [], []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        true_labels.extend(labels.cpu().numpy())\n        predicted_labels.extend(predicted.cpu().numpy())\n        predictions.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n\ntest_loss /= len(test_loader)\ntest_acc = 100. * correct / total\n\nprint(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n\ncm = confusion_matrix(true_labels, predicted_labels)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix - Test Set')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report - Test Set\")\nprint(classification_report(true_labels, predicted_labels, target_names=dataset.classes))\n\n# ROC/AUC Curve for Multiclass Classification\ntrue_labels_bin = label_binarize(true_labels, classes=np.arange(len(dataset.classes)))\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(len(dataset.classes)):\n    fpr[i], tpr[i], _ = roc_curve(true_labels_bin[:, i], np.array(predictions)[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curve for each class\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple']  # Add more colors if needed\nfor i, color in zip(range(len(dataset.classes)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, label=f'ROC curve (class {dataset.classes[i]}) (AUC = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Multiclass Classification - Test Set')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Misclassified Images\nmisclassified_idx = [i for i, (p, t) in enumerate(zip(predicted_labels, true_labels)) if p != t]\nfor idx in misclassified_idx[:10]:  # Limit to first 10 for visualization\n    true_label = dataset.classes[true_labels[idx]]\n    predicted_label = dataset.classes[predicted_labels[idx]]\n    probabilities = predictions[idx]\n\n    print(f\"\\nImage Index: {idx}\")\n    print(f\"True Label: {true_label}\")\n    print(f\"Predicted Label: {predicted_label}\")\n    print(f\"Class Probabilities: {probabilities}\")\n\n    # Visualize the misclassified image\n    img = Image.open(dataset.images[idx][0])\n    plt.imshow(img)\n    plt.title(f\"True: {true_label}, Pred: {predicted_label}\")\n    plt.xlabel(f\"Probs: {probabilities}\")\n    plt.axis('off')\n    plt.show()\n\n\n\n# Plot Train and Test Loss/Accuracy Graphs\nplt.figure(figsize=(12, 5))\n\n# Loss Curve\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train vs Test Loss')\nplt.legend()\n\n# Accuracy Curve\nplt.subplot(1, 2, 2)\nplt.plot(train_accs, label='Train Accuracy')\nplt.plot(test_accs, label='Test Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\nplt.legend()\n\nplt.show()\n\n# T-SNE Visualization on Test Set\ndef extract_embeddings(model, dataloader, device):\n    model.eval()\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            embeddings.extend(outputs.cpu().numpy())\n            labels.extend(targets.cpu().numpy())\n    return np.array(embeddings), np.array(labels)\n\n# Extract embeddings from the last layer\nembeddings, labels = extract_embeddings(model, test_loader, device)\n\n# Apply T-SNE\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Plot T-SNE\nplt.figure(figsize=(10, 8))\nfor i, cls in enumerate(dataset.classes):\n    idx = labels == i\n    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], label=cls)\nplt.xlabel('TSNE Component 1')\nplt.ylabel('TSNE Component 2')\nplt.title('T-SNE Visualization of Embeddings (Test Set)')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:43:47.457525Z","iopub.execute_input":"2025-03-16T18:43:47.457809Z","iopub.status.idle":"2025-03-16T19:00:48.782266Z","shell.execute_reply.started":"2025-03-16T18:43:47.457782Z","shell.execute_reply":"2025-03-16T19:00:48.781372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a download link for the file\nFileLink(r'mixeddataset_resnet_classweights.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:00:48.783094Z","iopub.execute_input":"2025-03-16T19:00:48.783344Z","iopub.status.idle":"2025-03-16T19:00:48.788172Z","shell.execute_reply.started":"2025-03-16T19:00:48.783304Z","shell.execute_reply":"2025-03-16T19:00:48.787548Z"}},"outputs":[],"execution_count":null}]}